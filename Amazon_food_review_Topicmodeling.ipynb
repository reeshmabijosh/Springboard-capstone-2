{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling  Amazon Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>buy several vitality dog food products find go...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arrive label jumbo salt peanuts peanut...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>look secret ingredient robitussin believe find...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                Summary  \\\n",
       "0      5  Good Quality Dog Food   \n",
       "1      1      Not as Advertised   \n",
       "2      4  \"Delight\" says it all   \n",
       "3      2         Cough Medicine   \n",
       "4      5            Great taffy   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I have bought several of the Vitality canned d...   \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  This is a confection that has been around a fe...   \n",
       "3  If you are looking for the secret ingredient i...   \n",
       "4  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                          clean_text  count  \n",
       "0  buy several vitality dog food products find go...     22  \n",
       "1  product arrive label jumbo salt peanuts peanut...     18  \n",
       "2  confection around centuries light pillowy citr...     39  \n",
       "3  look secret ingredient robitussin believe find...     18  \n",
       "4  great taffy great price wide assortment yummy ...     13  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('clean_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 393576 entries, 0 to 393575\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Score       393576 non-null  int64 \n",
      " 1   Summary     393576 non-null  object\n",
      " 2   Text        393576 non-null  object\n",
      " 3   clean_text  393573 non-null  object\n",
      " 4   count       393576 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393576, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Size of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unwanted columns\n",
    "df.drop(['count','Summary'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "df.drop_duplicates(subset=['Text'],inplace=True)  \n",
    "#dropping na\n",
    "df.dropna(axis=0,inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 393573 entries, 0 to 393575\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Score       393573 non-null  int64 \n",
      " 1   Text        393573 non-null  object\n",
      " 2   clean_text  393573 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393573, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final size of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample review\n",
    "df['Text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check the unique number of Score\n",
    "df['Score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    250713\n",
       "4     56042\n",
       "1     36275\n",
       "3     29752\n",
       "2     20791\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#value_counts of score\n",
    "df['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25fa03f6a20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#countplot of score\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Analysis of Score',fontsize=14)\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Score',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the countplot it is clear that most of the reviews have score 5 and very less reviwes have score 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling to Mining Amazon Reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Topic Modeling is a text mining approach. A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. \n",
    "  Topic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. \n",
    "    \n",
    "  Topic Modeling Algorithms\n",
    "  \n",
    "There are several algorithms for doing topic modeling. The most popular ones include\n",
    "LSA or LSI – Latent Semantic Analysis or Latent Semantic Indexing – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra\n",
    "\n",
    "NMF – Non-Negative Matrix Factorization – Based on Linear Algebra\n",
    "\n",
    "LDA – Latent Dirichlet Allocation – The one we’ll be focusing here. Its foundations are Probabilistic Graphical Models\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does LDA do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA is based upon two general assumptions:\n",
    "\n",
    "Documents that have similar words usually have the same topic\n",
    "\n",
    "Documents that have groups of words frequently occurring together usually have the same topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
    "A topic is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the keywords, you can identify what the topic is all about.\n",
    "\n",
    "I will be using the Latent Dirichlet Allocation (LDA) from Gensim package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['buy', 'several', 'vitality', 'dog', 'food', 'products', 'find', 'good', 'quality', 'product', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', 'labrador', 'finicky', 'appreciate', 'product', 'better']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df['clean_text']))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'several', 'vitality', 'dog', 'food', 'products', 'find', 'good', 'quality', 'product', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', 'labrador', 'finicky', 'appreciate', 'product', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['buy', 'several', 'vitality', 'dog', 'food', 'products', 'find', 'good', 'quality', 'product', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', 'labrador', 'finicky', 'appreciate', 'product', 'better']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_words_bigrams[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('appreciate', 1),\n",
       "  ('better', 2),\n",
       "  ('buy', 1),\n",
       "  ('dog', 1),\n",
       "  ('find', 1),\n",
       "  ('finicky', 1),\n",
       "  ('food', 1),\n",
       "  ('good', 1),\n",
       "  ('labrador', 1),\n",
       "  ('like', 1),\n",
       "  ('look', 1),\n",
       "  ('meat', 1),\n",
       "  ('process', 1),\n",
       "  ('product', 2),\n",
       "  ('products', 1),\n",
       "  ('quality', 1),\n",
       "  ('several', 1),\n",
       "  ('smell', 1),\n",
       "  ('stew', 1),\n",
       "  ('vitality', 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.041*\"like\" + 0.040*\"taste\" + 0.032*\"good\" + 0.027*\"flavor\" + 0.025*\"try\" + 0.024*\"would\" + 0.022*\"one\" + 0.017*\"tea\" + 0.015*\"really\" + 0.014*\"get\"'), (1, '0.033*\"love\" + 0.032*\"buy\" + 0.030*\"product\" + 0.028*\"great\" + 0.026*\"find\" + 0.020*\"order\" + 0.019*\"time\" + 0.018*\"get\" + 0.018*\"price\" + 0.017*\"box\"'), (2, '0.031*\"make\" + 0.016*\"eat\" + 0.014*\"add\" + 0.013*\"cookies\" + 0.011*\"sauce\" + 0.011*\"cookie\" + 0.011*\"mix\" + 0.009*\"sugar\" + 0.008*\"soft\" + 0.008*\"also\"'), (3, '0.068*\"coffee\" + 0.034*\"drink\" + 0.025*\"cup\" + 0.023*\"bean\" + 0.015*\"day\" + 0.013*\"roast\" + 0.013*\"strong\" + 0.011*\"blend\" + 0.011*\"dark\" + 0.009*\"morning\"'), (4, '0.043*\"food\" + 0.030*\"dog\" + 0.024*\"treat\" + 0.018*\"cat\" + 0.016*\"old\" + 0.015*\"eat\" + 0.012*\"hair\" + 0.009*\"salmon\" + 0.009*\"dry\" + 0.008*\"health\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      " like, 0.041212521493434906\n",
      " taste, 0.039847202599048615\n",
      " good, 0.03170450031757355\n",
      " flavor, 0.027232082560658455\n",
      " try, 0.02529573068022728\n",
      " would, 0.02414184808731079\n",
      " one, 0.022158855572342873\n",
      " tea, 0.017496056854724884\n",
      " really, 0.015455037355422974\n",
      " get, 0.014466571621596813\n",
      "\n",
      "Topic 2:\n",
      " love, 0.03268346190452576\n",
      " buy, 0.03201187774538994\n",
      " product, 0.030239162966609\n",
      " great, 0.028007538989186287\n",
      " find, 0.0256668608635664\n",
      " order, 0.019710730761289597\n",
      " time, 0.01923275738954544\n",
      " get, 0.018145060166716576\n",
      " price, 0.017944319173693657\n",
      " box, 0.016777878627181053\n",
      "\n",
      "Topic 3:\n",
      " make, 0.03073294460773468\n",
      " eat, 0.016306426376104355\n",
      " add, 0.013818901032209396\n",
      " cookies, 0.01323580089956522\n",
      " sauce, 0.01146496832370758\n",
      " cookie, 0.010840049013495445\n",
      " mix, 0.01054997555911541\n",
      " sugar, 0.009061293676495552\n",
      " soft, 0.007922213524580002\n",
      " also, 0.007645383011549711\n",
      "\n",
      "Topic 4:\n",
      " coffee, 0.06816712021827698\n",
      " drink, 0.033673178404569626\n",
      " cup, 0.025338539853692055\n",
      " bean, 0.022748488932847977\n",
      " day, 0.015229566022753716\n",
      " roast, 0.012668454088270664\n",
      " strong, 0.01253393106162548\n",
      " blend, 0.01112295314669609\n",
      " dark, 0.01071841735392809\n",
      " morning, 0.00901393499225378\n",
      "\n",
      "Topic 5:\n",
      " food, 0.043444521725177765\n",
      " dog, 0.029506206512451172\n",
      " treat, 0.023511335253715515\n",
      " cat, 0.018174858763813972\n",
      " old, 0.015771476551890373\n",
      " eat, 0.01544055063277483\n",
      " hair, 0.012489150278270245\n",
      " salmon, 0.009166519157588482\n",
      " dry, 0.008983378298580647\n",
      " health, 0.008363571017980576\n"
     ]
    }
   ],
   "source": [
    "for topic in range(0, 5):\n",
    "    print(f\"\\nTopic {topic+1}:\")\n",
    "    for token, frequency in lda_model.show_topic(topic, topn=10):\n",
    "        print(f\" {token}, {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1846026125580600647917570858\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1846026125580600647917570858_data = {\"mdsDat\": {\"x\": [0.26242215221889015, -0.17016672639044625, 0.20625324940347003, 0.01629520663933377, -0.3148038818712475], \"y\": [0.27267552344957097, -0.06308622079016617, -0.30304571048019807, 0.02909500331798168, 0.06436140450281176], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [32.10765838623047, 24.57712745666504, 23.974014282226562, 9.852239608764648, 9.488969802856445]}, \"tinfo\": {\"Term\": [\"coffee\", \"like\", \"taste\", \"good\", \"buy\", \"product\", \"food\", \"try\", \"make\", \"love\", \"find\", \"would\", \"flavor\", \"great\", \"drink\", \"order\", \"dog\", \"eat\", \"tea\", \"price\", \"box\", \"cup\", \"amazon\", \"really\", \"bag\", \"time\", \"treat\", \"one\", \"bean\", \"much\", \"like\", \"taste\", \"good\", \"try\", \"tea\", \"really\", \"much\", \"think\", \"say\", \"better\", \"want\", \"sweet\", \"nice\", \"stuff\", \"something\", \"review\", \"bad\", \"definitely\", \"different\", \"though\", \"pretty\", \"big\", \"thing\", \"far\", \"star\", \"people\", \"probably\", \"things\", \"nothing\", \"might\", \"would\", \"bottle\", \"flavor\", \"brand\", \"even\", \"one\", \"hot\", \"lot\", \"first\", \"give\", \"recommend\", \"bite\", \"little\", \"enjoy\", \"get\", \"look\", \"know\", \"still\", \"add\", \"cookies\", \"sauce\", \"mix\", \"cookie\", \"salt\", \"oil\", \"easy\", \"tasty\", \"bake\", \"cook\", \"organic\", \"serve\", \"popcorn\", \"oatmeal\", \"fruit\", \"cheese\", \"texture\", \"expect\", \"heat\", \"calories\", \"rice\", \"soup\", \"pasta\", \"low\", \"top\", \"fine\", \"fat\", \"chewy\", \"quaker\", \"make\", \"sugar\", \"put\", \"eat\", \"free\", \"soft\", \"need\", \"water\", \"delicious\", \"milk\", \"whole\", \"also\", \"great\", \"dry\", \"find\", \"order\", \"price\", \"box\", \"amazon\", \"store\", \"package\", \"chocolate\", \"purchase\", \"pack\", \"ship\", \"receive\", \"bar\", \"candy\", \"local\", \"arrive\", \"grocery\", \"thank\", \"case\", \"gift\", \"pay\", \"sell\", \"able\", \"item\", \"large\", \"send\", \"piece\", \"deal\", \"carry\", \"chew\", \"buy\", \"product\", \"bag\", \"snack\", \"time\", \"love\", \"open\", \"always\", \"great\", \"quality\", \"years\", \"hard\", \"back\", \"get\", \"come\", \"best\", \"keep\", \"could\", \"food\", \"dog\", \"treat\", \"cat\", \"old\", \"hair\", \"salmon\", \"health\", \"energy\", \"protein\", \"shampoo\", \"week\", \"baby\", \"formula\", \"problems\", \"teeth\", \"pet\", \"vet\", \"feed\", \"lose\", \"bone\", \"scalp\", \"skin\", \"cause\", \"fee\", \"dandruff\", \"weight\", \"vitamins\", \"crazy\", \"wet\", \"issue\", \"clean\", \"contain\", \"switch\", \"help\", \"start\", \"dry\", \"eat\", \"work\", \"day\", \"healthy\", \"give\", \"foods\", \"seem\", \"take\", \"ingredients\", \"coffee\", \"drink\", \"cup\", \"bean\", \"roast\", \"strong\", \"blend\", \"dark\", \"morning\", \"brew\", \"bitter\", \"grind\", \"either\", \"cereal\", \"smooth\", \"house\", \"rich\", \"espresso\", \"raisins\", \"vanilla\", \"dish\", \"glad\", \"red\", \"aroma\", \"result\", \"french\", \"shop\", \"pot\", \"due\", \"caffeine\", \"jelly_belly\", \"usually\", \"cost\", \"day\", \"absolutely\", \"full\", \"make\", \"water\"], \"Freq\": [95494.0, 195353.0, 188881.0, 150283.0, 113346.0, 107985.0, 63191.0, 119905.0, 123066.0, 131104.0, 90844.0, 114511.0, 135486.0, 118302.0, 47172.0, 69763.0, 42917.0, 81624.0, 82934.0, 63511.0, 59383.0, 35496.0, 58398.0, 73259.0, 58606.0, 73030.0, 34198.0, 123136.0, 31868.0, 64680.0, 195352.203125, 188880.4375, 150283.0625, 119904.734375, 82933.3671875, 73258.6953125, 64679.96484375, 61347.6953125, 51640.89453125, 49721.96484375, 43018.296875, 35919.1953125, 34240.3203125, 29312.134765625, 27344.171875, 26017.58203125, 23848.29296875, 23201.05859375, 23187.955078125, 22897.64453125, 21537.404296875, 19057.521484375, 19021.287109375, 18089.736328125, 17084.80078125, 16569.390625, 15562.76171875, 14114.4873046875, 13173.5234375, 13002.943359375, 114435.1953125, 28645.01171875, 129083.2734375, 33669.359375, 49151.1875, 105035.5859375, 35878.08984375, 28298.615234375, 36939.19140625, 44465.97265625, 33495.92578125, 34498.796875, 43632.4453125, 28863.080078125, 68573.25, 35549.68359375, 32431.01171875, 29154.603515625, 50140.078125, 48024.375, 41599.140625, 38279.2109375, 39331.703125, 24918.65234375, 21135.1484375, 21014.193359375, 20268.8828125, 19931.880859375, 19704.009765625, 19648.521484375, 18937.951171875, 18475.53125, 16774.634765625, 16068.189453125, 16002.462890625, 15533.2666015625, 15702.8427734375, 14831.1318359375, 14614.3916015625, 14396.876953125, 14344.1767578125, 13035.2177734375, 13187.7177734375, 12761.5556640625, 12901.0771484375, 12497.533203125, 12294.068359375, 12565.328125, 111510.4765625, 32877.72265625, 24411.8359375, 59165.73828125, 24659.2734375, 28744.71875, 26202.998046875, 24962.041015625, 23547.890625, 20384.853515625, 18846.716796875, 27740.275390625, 19173.7421875, 16453.12890625, 90843.4921875, 69762.78125, 63510.86328125, 59382.4453125, 58397.3515625, 51100.61328125, 39922.78125, 38810.14453125, 37949.4765625, 32955.21484375, 27658.8125, 25527.78515625, 22052.98046875, 19818.59375, 19557.67578125, 17781.65625, 16423.61328125, 15835.8369140625, 15257.6767578125, 14838.166015625, 14327.923828125, 14312.2373046875, 12691.107421875, 12619.103515625, 12443.6630859375, 12092.25390625, 11723.12109375, 11368.33984375, 11097.5126953125, 10883.208984375, 113300.5859375, 107026.375, 57789.21875, 30325.48828125, 68071.0703125, 115677.546875, 20663.572265625, 22752.71484375, 99127.9140625, 22155.162109375, 22875.916015625, 20062.40234375, 20411.75, 64221.3515625, 30804.744140625, 25288.77734375, 21625.294921875, 22384.626953125, 63190.28515625, 42916.93359375, 34197.359375, 26435.427734375, 22939.6953125, 18165.53515625, 13332.75, 12164.857421875, 10167.541015625, 10207.8056640625, 9425.736328125, 9284.189453125, 8951.74609375, 7717.8173828125, 6799.10693359375, 6018.130859375, 5562.35791015625, 5428.5146484375, 5467.34814453125, 5136.52783203125, 5050.88525390625, 5880.3369140625, 4879.13232421875, 4750.8173828125, 4520.27587890625, 4478.17236328125, 4237.44580078125, 3883.38623046875, 3542.009765625, 3433.616943359375, 7403.48046875, 8784.984375, 11562.8037109375, 5549.05322265625, 12149.4052734375, 11334.673828125, 13066.37109375, 22458.36328125, 11993.5556640625, 11073.123046875, 9003.484375, 11388.9990234375, 7065.3203125, 7957.72705078125, 7464.82177734375, 6502.037109375, 95493.609375, 47171.91015625, 35496.125, 31867.787109375, 17746.919921875, 17558.470703125, 15581.8662109375, 15015.162109375, 12627.3955078125, 12420.7578125, 11904.6884765625, 11405.791015625, 11583.2021484375, 10541.2587890625, 10525.357421875, 10727.818359375, 9960.302734375, 9579.6591796875, 10309.58203125, 9309.6162109375, 8513.1552734375, 8239.0068359375, 8253.4052734375, 8092.3671875, 8081.0810546875, 7138.30322265625, 6859.2939453125, 6763.54638671875, 6683.046875, 6298.92724609375, 6918.53466796875, 12078.84765625, 12327.716796875, 21334.716796875, 10163.0302734375, 9399.7470703125, 11554.9345703125, 9431.3671875], \"Total\": [95494.0, 195353.0, 188881.0, 150283.0, 113346.0, 107985.0, 63191.0, 119905.0, 123066.0, 131104.0, 90844.0, 114511.0, 135486.0, 118302.0, 47172.0, 69763.0, 42917.0, 81624.0, 82934.0, 63511.0, 59383.0, 35496.0, 58398.0, 73259.0, 58606.0, 73030.0, 34198.0, 123136.0, 31868.0, 64680.0, 195353.09375, 188881.328125, 150283.953125, 119905.6171875, 82934.234375, 73259.578125, 64680.8515625, 61348.58203125, 51641.78125, 49722.8515625, 43019.18359375, 35920.07421875, 34241.20703125, 29313.017578125, 27345.0546875, 26018.466796875, 23849.17578125, 23201.943359375, 23188.841796875, 22898.52734375, 21538.28515625, 19058.404296875, 19022.169921875, 18090.623046875, 17085.685546875, 16570.2734375, 15563.646484375, 14115.3720703125, 13174.408203125, 13003.8271484375, 114511.28125, 28657.509765625, 135486.25, 35342.703125, 53389.90625, 123136.09375, 40050.87890625, 30529.35546875, 42797.4453125, 55855.62890625, 38510.1875, 40211.1015625, 58129.00390625, 31849.41796875, 137304.640625, 45474.609375, 39469.07421875, 32605.67578125, 50140.96484375, 48025.27734375, 41600.0390625, 38280.09765625, 39332.61328125, 24919.5390625, 21136.037109375, 21015.0859375, 20269.7734375, 19932.767578125, 19704.890625, 19649.4140625, 18938.83984375, 18476.41796875, 16775.52734375, 16069.0732421875, 16003.3525390625, 15534.1533203125, 15703.740234375, 14832.0283203125, 14615.2783203125, 14397.76171875, 14345.060546875, 13036.1025390625, 13188.61328125, 12762.447265625, 12901.9814453125, 12498.4189453125, 12294.96484375, 12566.248046875, 123066.0859375, 37232.27734375, 27526.611328125, 81624.7578125, 28375.759765625, 35763.25390625, 33161.7578125, 34394.06640625, 33866.7421875, 26702.0546875, 24852.3515625, 59933.109375, 118302.328125, 29520.158203125, 90844.375, 69763.65625, 63511.75, 59383.33203125, 58398.2265625, 51101.4921875, 39923.6640625, 38811.02734375, 37950.36328125, 32956.09765625, 27659.69140625, 25528.66796875, 22053.865234375, 19819.470703125, 19558.556640625, 17782.537109375, 16424.4921875, 15836.7177734375, 15258.5625, 14839.044921875, 14328.8056640625, 14313.12109375, 12691.9921875, 12619.982421875, 12444.552734375, 12093.1357421875, 11724.005859375, 11369.22265625, 11098.3935546875, 10884.09375, 113346.5859375, 107985.4296875, 58606.328125, 30693.341796875, 73030.3359375, 131104.390625, 21318.53125, 23844.19921875, 118302.328125, 23828.0546875, 25435.736328125, 21910.20703125, 22693.5703125, 137304.640625, 45143.63671875, 54324.1875, 31306.603515625, 43107.578125, 63191.15234375, 42917.79296875, 34198.2265625, 26436.283203125, 22940.57421875, 18166.40234375, 13333.65234375, 12165.7294921875, 10168.408203125, 10208.6865234375, 9426.6083984375, 9285.0732421875, 8952.6083984375, 7718.67626953125, 6799.9755859375, 6018.994140625, 5563.21337890625, 5429.36865234375, 5468.20947265625, 5137.40087890625, 5051.74365234375, 5881.3779296875, 4879.998046875, 4751.6904296875, 4521.13623046875, 4479.06298828125, 4238.30859375, 3884.256103515625, 3542.895751953125, 3434.48486328125, 7490.27099609375, 9649.12890625, 14004.90625, 5907.96044921875, 16812.876953125, 20752.7734375, 29520.158203125, 81624.7578125, 35675.23046875, 32408.486328125, 24177.609375, 55855.62890625, 13781.7578125, 29339.609375, 39018.94921875, 19613.583984375, 95494.46875, 47172.7734375, 35496.98828125, 31868.65234375, 17747.783203125, 17559.3359375, 15582.7314453125, 15016.0302734375, 12628.2626953125, 12421.615234375, 11905.5517578125, 11406.6591796875, 11584.0869140625, 10542.1279296875, 10526.2236328125, 10728.703125, 9961.16796875, 9580.517578125, 10310.515625, 9310.482421875, 8514.0458984375, 8239.8818359375, 8254.28515625, 8093.232421875, 8081.96826171875, 7139.1669921875, 6860.17236328125, 6764.4189453125, 6683.9453125, 6299.7880859375, 6919.7705078125, 13768.958984375, 14791.5322265625, 32408.486328125, 12635.59765625, 14994.986328125, 123066.0859375, 34394.06640625], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.188999891281128, -3.2227001190185547, -3.4512999057769775, -3.6770999431610107, -4.04580020904541, -4.1697998046875, -4.294400215148926, -4.347300052642822, -4.519499778747559, -4.557400226593018, -4.702199935913086, -4.882500171661377, -4.9303998947143555, -5.0858001708984375, -5.155300140380859, -5.204999923706055, -5.292099952697754, -5.3196001052856445, -5.320199966430664, -5.332799911499023, -5.394000053405762, -5.51639986038208, -5.5183000564575195, -5.56850004196167, -5.6255998611450195, -5.656300067901611, -5.718900203704834, -5.8165998458862305, -5.8856000900268555, -5.898600101470947, -3.72379994392395, -5.108799934387207, -3.6033999919891357, -4.947199821472168, -4.568900108337402, -3.809499979019165, -4.883699893951416, -5.120999813079834, -4.854499816894531, -4.669099807739258, -4.952400207519531, -4.922900199890137, -4.688000202178955, -5.10129976272583, -4.235899925231934, -4.892899990081787, -4.9847002029418945, -5.09119987487793, -4.281700134277344, -4.32480001449585, -4.468500137329102, -4.551599979400635, -4.524499893188477, -4.980899810791016, -5.145599842071533, -5.151299953460693, -5.1875, -5.20419979095459, -5.215700149536133, -5.218500137329102, -5.25540018081665, -5.280099868774414, -5.376699924468994, -5.4197001457214355, -5.423799991607666, -5.45359992980957, -5.442699909210205, -5.499800205230713, -5.514500141143799, -5.5295000076293945, -5.533199787139893, -5.628900051116943, -5.617300033569336, -5.650100231170654, -5.639200210571289, -5.671000003814697, -5.687399864196777, -5.665599822998047, -3.4823999404907227, -4.703700065612793, -5.001500129699707, -4.116199970245361, -4.991399765014648, -4.838099956512451, -4.930699825286865, -4.9791998863220215, -5.037499904632568, -5.181700229644775, -5.260200023651123, -4.873700141906738, -5.243000030517578, -5.395999908447266, -3.662600040435791, -3.9265999794006348, -4.020500183105469, -4.087699890136719, -4.104400157928467, -4.2378997802734375, -4.4847002029418945, -4.513000011444092, -4.535399913787842, -4.676499843597412, -4.8516998291015625, -4.9319000244140625, -5.078199863433838, -5.185100078582764, -5.198299884796143, -5.293499946594238, -5.373000144958496, -5.40939998626709, -5.446599960327148, -5.4745001792907715, -5.509500026702881, -5.5106000900268555, -5.630799770355225, -5.636499881744385, -5.6504998207092285, -5.679100036621094, -5.710100173950195, -5.740900039672852, -5.764999866485596, -5.7845001220703125, -3.4416000843048096, -3.4986000061035156, -4.1149001121521, -4.759699821472168, -3.9511001110076904, -3.4209001064300537, -5.1433000564575195, -5.046999931335449, -3.5752999782562256, -5.073599815368652, -5.041600227355957, -5.172800064086914, -5.155600070953369, -4.009399890899658, -4.74399995803833, -4.941299915313721, -5.097799777984619, -5.063300132751465, -3.1363000869750977, -3.523200035095215, -3.7502999305725098, -4.007699966430664, -4.149600028991699, -4.382900238037109, -4.692200183868408, -4.783899784088135, -4.963200092315674, -4.9593000411987305, -5.039000034332275, -5.054100036621094, -5.09060001373291, -5.238900184631348, -5.365600109100342, -5.487599849700928, -5.566400051116943, -5.590799808502197, -5.583600044250488, -5.645999908447266, -5.662899971008301, -5.510799884796143, -5.697500228881836, -5.724100112915039, -5.773799896240234, -5.783199787139893, -5.838500022888184, -5.9257001876831055, -6.0177001953125, -6.048799991607666, -5.2804999351501465, -5.109399795532227, -4.83459997177124, -5.56879997253418, -4.785099983215332, -4.854599952697754, -4.712399959564209, -4.17080020904541, -4.798099994659424, -4.877900123596191, -5.084799766540527, -4.849800109863281, -5.327199935913086, -5.2083001136779785, -5.272200107574463, -5.410299777984619, -2.685800075531006, -3.3910999298095703, -3.6754000186920166, -3.7832999229431152, -4.368599891662598, -4.379300117492676, -4.498700141906738, -4.535799980163574, -4.709000110626221, -4.725500106811523, -4.767899990081787, -4.810699939727783, -4.795300006866455, -4.889599800109863, -4.89109992980957, -4.872000217437744, -4.946199893951416, -4.985199928283691, -4.911799907684326, -5.013800144195557, -5.1031999588012695, -5.136000156402588, -5.134200096130371, -5.153900146484375, -5.155300140380859, -5.279399871826172, -5.319200038909912, -5.3333001136779785, -5.345300197601318, -5.4045000076293945, -5.3105998039245605, -4.753399848937988, -4.732999801635742, -4.184500217437744, -4.92609977722168, -5.004199981689453, -4.797699928283691, -5.000800132751465], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1361000537872314, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1360000371932983, 1.1354000568389893, 1.135599970817566, 1.0877000093460083, 1.0875999927520752, 1.0534000396728516, 0.9771000146865845, 1.0261000394821167, 1.0601999759674072, 0.9889000058174133, 0.9079999923706055, 0.9965999722480774, 0.9829000234603882, 0.8492000102996826, 1.037600040435791, 0.44179999828338623, 0.8899000287055969, 0.9397000074386597, 1.0241999626159668, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.4033000469207764, 1.304800033569336, 1.2790000438690186, 1.2833000421524048, 1.0815999507904053, 1.2630000114440918, 1.1849000453948975, 1.167799949645996, 1.082800030708313, 1.0399999618530273, 1.1333999633789062, 1.1267000436782837, 0.6330000162124634, -0.4162999987602234, 0.8187999725341797, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4282000064849854, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4280999898910522, 1.4278000593185425, 1.4192999601364136, 1.414199948310852, 1.416100025177002, 1.3579000234603882, 1.3029999732971191, 1.3969999551773071, 1.3812999725341797, 1.2513999938964844, 1.3553999662399292, 1.322100043296814, 1.3401000499725342, 1.3221999406814575, 0.6682999730110168, 1.0460000038146973, 0.6636000275611877, 1.0582000017166138, 0.7728999853134155, 2.317500114440918, 2.317500114440918, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173999786376953, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.3173000812530518, 2.317199945449829, 2.317199945449829, 2.317199945449829, 2.305799961090088, 2.223599910736084, 2.1259000301361084, 2.2548000812530518, 1.9925999641418457, 1.7127000093460083, 1.5024000406265259, 1.0269999504089355, 1.2273999452590942, 1.2436000108718872, 1.329699993133545, 0.7272999882698059, 1.6492999792099, 1.0126999616622925, 0.6636000275611877, 1.2134000062942505, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3550000190734863, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.3548998832702637, 2.224100112915039, 2.172800064086914, 1.937000036239624, 2.1373000144958496, 1.8880000114440918, -0.010599999688565731, 1.0612000226974487]}, \"token.table\": {\"Topic\": [3, 4, 5, 2, 1, 2, 4, 2, 3, 4, 3, 5, 3, 4, 3, 4, 1, 1, 3, 2, 3, 5, 1, 2, 3, 5, 1, 1, 1, 2, 5, 5, 4, 1, 2, 3, 1, 4, 5, 1, 3, 5, 2, 3, 3, 3, 4, 4, 5, 2, 3, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 2, 2, 2, 4, 5, 1, 3, 4, 5, 4, 5, 4, 5, 3, 1, 2, 3, 1, 5, 4, 5, 2, 4, 5, 2, 2, 4, 5, 4, 1, 5, 5, 1, 2, 3, 4, 2, 1, 2, 4, 4, 3, 2, 1, 3, 4, 1, 5, 4, 2, 4, 4, 2, 3, 4, 5, 2, 2, 4, 5, 1, 3, 4, 3, 1, 4, 5, 1, 2, 3, 5, 3, 4, 1, 3, 4, 1, 4, 2, 1, 4, 1, 2, 5, 2, 4, 4, 5, 3, 5, 2, 3, 4, 1, 3, 4, 3, 1, 1, 2, 3, 1, 3, 4, 4, 1, 2, 4, 1, 3, 4, 2, 2, 5, 1, 2, 5, 2, 5, 1, 2, 4, 5, 1, 1, 2, 2, 4, 1, 3, 4, 1, 3, 3, 2, 3, 3, 2, 3, 1, 4, 3, 2, 5, 1, 3, 1, 4, 2, 3, 4, 3, 2, 4, 2, 1, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 5, 1, 2, 5, 5, 4, 2, 2, 1, 4, 1, 4, 3, 3, 2, 4, 3, 5, 4, 5, 1, 3, 2, 3, 4, 1, 2, 1, 2, 4, 5, 1, 3, 4, 3, 5, 1, 1, 2, 1, 4, 5, 1, 2, 3, 4, 1, 2, 1, 4, 2, 3, 1, 1, 1, 1, 2, 3, 4, 2, 4, 1, 4, 5, 5, 4, 4, 1, 2, 5, 4, 4, 4, 2, 3, 5, 2, 4, 5, 1, 3, 3, 4], \"Freq\": [0.9999217987060547, 0.1956377625465393, 0.8043149709701538, 0.9999807476997375, 0.4322151839733124, 0.46284934878349304, 0.10491696745157242, 0.03711594641208649, 0.9542362689971924, 0.008639417588710785, 0.9999790191650391, 0.9998477101325989, 0.9999697804450989, 0.999932050704956, 0.8994618058204651, 0.1005130484700203, 0.9999507069587708, 0.013923411257565022, 0.9860539436340332, 0.9999614953994751, 0.9999607801437378, 0.999979555606842, 0.5231555700302124, 0.010934355668723583, 0.46552008390426636, 0.0003865681355819106, 0.9999828934669495, 0.99997878074646, 0.8579471707344055, 0.14205032587051392, 0.9999536275863647, 0.999953031539917, 0.9998527765274048, 0.999563455581665, 0.0004187384038232267, 0.9999775886535645, 0.9526435732841492, 0.047336503863334656, 0.9999504685401917, 0.000397012394387275, 0.9995978474617004, 0.9998748898506165, 0.9999125599861145, 0.9999762773513794, 0.9999645352363586, 0.9999631643295288, 0.9999514818191528, 0.9998546838760376, 0.9998930096626282, 0.9999154806137085, 0.9998995065689087, 0.9999215006828308, 0.9999735355377197, 0.9104448556900024, 0.08954176306724548, 0.9999951124191284, 0.3128458559513092, 0.004762575961649418, 0.6823774576187134, 0.8256392478942871, 0.1742960661649704, 0.9999548196792603, 0.9999843835830688, 0.9999734163284302, 0.16651418805122375, 0.8334498405456543, 0.48070433735847473, 0.5192822217941284, 0.9997471570968628, 0.9999721646308899, 0.9997626543045044, 0.9999313950538635, 0.3416697680950165, 0.6583152413368225, 0.9998924732208252, 0.9999593496322632, 0.6953133940696716, 0.304664671421051, 0.9999637007713318, 0.9998771548271179, 0.9999815225601196, 0.9999836087226868, 0.5573479533195496, 0.44261279702186584, 0.999858558177948, 0.9999483227729797, 0.7248536348342896, 0.2751370966434479, 0.9999061822891235, 0.9999598264694214, 0.9062331914901733, 0.09375367313623428, 0.9999459981918335, 0.9206047058105469, 0.03159773349761963, 0.031504083424806595, 0.016295215114951134, 0.9999528527259827, 0.9999655485153198, 0.9999665021896362, 0.999748706817627, 0.9997788071632385, 0.999984860420227, 0.9999239444732666, 0.8631122708320618, 0.11568447202444077, 0.02116948738694191, 0.9527387619018555, 0.04725202918052673, 0.9999817609786987, 0.48731085658073425, 0.5126341581344604, 0.9999123811721802, 0.8690163493156433, 0.10350383818149567, 0.02745300903916359, 0.9998365640640259, 0.9999331831932068, 0.329376757144928, 0.043747954070568085, 0.626876175403595, 0.4994223117828369, 0.46772634983062744, 0.03284666687250137, 0.9999296069145203, 0.7960880994796753, 0.20390066504478455, 0.9998929500579834, 0.9999936819076538, 0.16207626461982727, 0.8379209637641907, 0.9999421834945679, 0.999970018863678, 0.9999778270721436, 0.08429861068725586, 0.9156463146209717, 0.999940037727356, 0.6275641322135925, 0.37236931920051575, 0.9999306797981262, 0.2773469388484955, 0.7226008772850037, 0.8958105444908142, 0.10416749864816666, 0.9999344348907471, 0.668465256690979, 0.3315049409866333, 0.9883487820625305, 0.01148156076669693, 0.9999221563339233, 0.9998886585235596, 0.13275153934955597, 0.6907488107681274, 0.17648033797740936, 0.8216813206672668, 0.09199607372283936, 0.08629541099071503, 0.9999555945396423, 0.9999943971633911, 0.7506063580513, 0.24937637150287628, 0.9999715685844421, 0.7817549109458923, 0.20398196578025818, 0.014271700754761696, 0.999921977519989, 0.9269438982009888, 0.06852421164512634, 0.004520239774137735, 0.09461162984371185, 0.8823350667953491, 0.023057961836457253, 0.9999535083770752, 0.9060985445976257, 0.0938926413655281, 0.9999364018440247, 0.7634243965148926, 0.23657356202602386, 0.999971330165863, 0.9998999834060669, 0.9999868273735046, 0.7901571393013, 0.12432996183633804, 0.0855201929807663, 0.9999647736549377, 0.9999690055847168, 0.9999685883522034, 0.9999509453773499, 0.9999749660491943, 0.853007435798645, 0.1327880322933197, 0.014203796163201332, 0.030677534639835358, 0.9692975282669067, 0.9999905824661255, 0.9999788999557495, 0.9999666810035706, 0.9999833703041077, 0.9999154210090637, 0.9999437928199768, 0.9999231696128845, 0.9997819066047668, 0.9999142289161682, 0.9999773502349854, 0.9999380707740784, 0.9999403357505798, 0.9999881982803345, 0.9999584555625916, 0.9998565316200256, 0.00887156743556261, 0.9911152124404907, 0.9999327659606934, 0.9999640583992004, 0.886850893497467, 0.11312688887119293, 0.9999006986618042, 0.01733251102268696, 0.9297863245010376, 0.05287884548306465, 0.9999499917030334, 0.9999921321868896, 0.9999738335609436, 0.8697957992553711, 0.046974584460258484, 0.028044527396559715, 0.055180203169584274, 0.9998443126678467, 0.9998801946640015, 0.9999820590019226, 0.9999470710754395, 0.9998827576637268, 0.9999558925628662, 0.9999510645866394, 0.9999783635139465, 0.9999750256538391, 0.999984860420227, 0.9997656941413879, 0.7287418246269226, 0.27123743295669556, 0.9999216794967651, 0.999906063079834, 0.9999556541442871, 0.9999354481697083, 0.9999750256538391, 0.999829113483429, 0.9997954964637756, 0.999883770942688, 0.011956990696489811, 0.9879993200302124, 0.8037579655647278, 0.10096956044435501, 0.09526538103818893, 0.9999614357948303, 0.9999260902404785, 0.9999598860740662, 0.1808432936668396, 0.5461920499801636, 0.2729755640029907, 0.8941694498062134, 0.08740809559822083, 0.018401704728603363, 0.9999903440475464, 0.9999239444732666, 0.9999653100967407, 0.11694154143333435, 0.8830509781837463, 0.9999700784683228, 0.9392412304878235, 0.06059620901942253, 0.6188531517982483, 0.10138663649559021, 0.08841858059167862, 0.19131730496883392, 0.9999929666519165, 0.9999618530273438, 0.9999850988388062, 0.9998348355293274, 0.9999257326126099, 0.9999547004699707, 0.9999384880065918, 0.9999027848243713, 0.9999905228614807, 0.9999769926071167, 0.0407228022813797, 0.9320921301841736, 0.027180485427379608, 0.9999649524688721, 0.9999641180038452, 0.9999948740005493, 0.12266722321510315, 0.8772631287574768, 0.9999482035636902, 0.9999321103096008, 0.9996766448020935, 0.9999724626541138, 0.7257646918296814, 0.274204283952713, 0.9998844265937805, 0.9996912479400635, 0.9998587965965271, 0.7583588361740112, 0.22050227224826813, 0.021124761551618576, 0.4336341917514801, 0.33619964122772217, 0.2301596850156784, 0.9993338584899902, 0.0006549573154188693, 0.8993645906448364, 0.10060648620128632], \"Term\": [\"able\", \"absolutely\", \"absolutely\", \"add\", \"also\", \"also\", \"also\", \"always\", \"always\", \"always\", \"amazon\", \"aroma\", \"arrive\", \"baby\", \"back\", \"back\", \"bad\", \"bag\", \"bag\", \"bake\", \"bar\", \"bean\", \"best\", \"best\", \"best\", \"best\", \"better\", \"big\", \"bite\", \"bite\", \"bitter\", \"blend\", \"bone\", \"bottle\", \"bottle\", \"box\", \"brand\", \"brand\", \"brew\", \"buy\", \"buy\", \"caffeine\", \"calories\", \"candy\", \"carry\", \"case\", \"cat\", \"cause\", \"cereal\", \"cheese\", \"chew\", \"chewy\", \"chocolate\", \"clean\", \"clean\", \"coffee\", \"come\", \"come\", \"come\", \"contain\", \"contain\", \"cook\", \"cookie\", \"cookies\", \"cost\", \"cost\", \"could\", \"could\", \"crazy\", \"cup\", \"dandruff\", \"dark\", \"day\", \"day\", \"deal\", \"definitely\", \"delicious\", \"delicious\", \"different\", \"dish\", \"dog\", \"drink\", \"dry\", \"dry\", \"due\", \"easy\", \"eat\", \"eat\", \"either\", \"energy\", \"enjoy\", \"enjoy\", \"espresso\", \"even\", \"even\", \"even\", \"even\", \"expect\", \"far\", \"fat\", \"fee\", \"feed\", \"find\", \"fine\", \"first\", \"first\", \"first\", \"flavor\", \"flavor\", \"food\", \"foods\", \"foods\", \"formula\", \"free\", \"free\", \"free\", \"french\", \"fruit\", \"full\", \"full\", \"full\", \"get\", \"get\", \"get\", \"gift\", \"give\", \"give\", \"glad\", \"good\", \"great\", \"great\", \"grind\", \"grocery\", \"hair\", \"hard\", \"hard\", \"health\", \"healthy\", \"healthy\", \"heat\", \"help\", \"help\", \"hot\", \"hot\", \"house\", \"ingredients\", \"ingredients\", \"issue\", \"issue\", \"item\", \"jelly_belly\", \"keep\", \"keep\", \"keep\", \"know\", \"know\", \"know\", \"large\", \"like\", \"little\", \"little\", \"local\", \"look\", \"look\", \"look\", \"lose\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"low\", \"make\", \"make\", \"might\", \"milk\", \"milk\", \"mix\", \"morning\", \"much\", \"need\", \"need\", \"need\", \"nice\", \"nothing\", \"oatmeal\", \"oil\", \"old\", \"one\", \"one\", \"one\", \"open\", \"open\", \"order\", \"organic\", \"pack\", \"package\", \"pasta\", \"pay\", \"people\", \"pet\", \"piece\", \"popcorn\", \"pot\", \"pretty\", \"price\", \"probably\", \"problems\", \"product\", \"product\", \"protein\", \"purchase\", \"put\", \"put\", \"quaker\", \"quality\", \"quality\", \"quality\", \"raisins\", \"really\", \"receive\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"red\", \"result\", \"review\", \"rice\", \"rich\", \"roast\", \"salmon\", \"salt\", \"sauce\", \"say\", \"scalp\", \"seem\", \"seem\", \"sell\", \"send\", \"serve\", \"shampoo\", \"ship\", \"shop\", \"skin\", \"smooth\", \"snack\", \"snack\", \"soft\", \"soft\", \"soft\", \"something\", \"soup\", \"star\", \"start\", \"start\", \"start\", \"still\", \"still\", \"still\", \"store\", \"strong\", \"stuff\", \"sugar\", \"sugar\", \"sweet\", \"switch\", \"switch\", \"take\", \"take\", \"take\", \"take\", \"taste\", \"tasty\", \"tea\", \"teeth\", \"texture\", \"thank\", \"thing\", \"things\", \"think\", \"though\", \"time\", \"time\", \"time\", \"top\", \"treat\", \"try\", \"usually\", \"usually\", \"vanilla\", \"vet\", \"vitamins\", \"want\", \"water\", \"water\", \"week\", \"weight\", \"wet\", \"whole\", \"whole\", \"whole\", \"work\", \"work\", \"work\", \"would\", \"would\", \"years\", \"years\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2, 5, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1846026125580600647917570858\", ldavis_el1846026125580600647917570858_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1846026125580600647917570858\", ldavis_el1846026125580600647917570858_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1846026125580600647917570858\", ldavis_el1846026125580600647917570858_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.262422  0.272676       1        1  32.107658\n",
       "2     -0.170167 -0.063086       2        1  24.577127\n",
       "1      0.206253 -0.303046       3        1  23.974014\n",
       "4      0.016295  0.029095       4        1   9.852240\n",
       "3     -0.314804  0.064361       5        1   9.488970, topic_info=            Term           Freq          Total Category  logprob  loglift\n",
       "391       coffee   95494.000000   95494.000000  Default  30.0000  30.0000\n",
       "9           like  195353.000000  195353.000000  Default  29.0000  29.0000\n",
       "196        taste  188881.000000  188881.000000  Default  28.0000  28.0000\n",
       "7           good  150283.000000  150283.000000  Default  27.0000  27.0000\n",
       "2            buy  113346.000000  113346.000000  Default  26.0000  26.0000\n",
       "...          ...            ...            ...      ...      ...      ...\n",
       "522          day   21334.716797   32408.486328   Topic5  -4.1845   1.9370\n",
       "1177  absolutely   10163.030273   12635.597656   Topic5  -4.9261   2.1373\n",
       "229         full    9399.747070   14994.986328   Topic5  -5.0042   1.8880\n",
       "77          make   11554.934570  123066.085938   Topic5  -4.7977  -0.0106\n",
       "414        water    9431.367188   34394.066406   Topic5  -5.0008   1.0612\n",
       "\n",
       "[254 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "344       3  0.999922        able\n",
       "1177      4  0.195638  absolutely\n",
       "1177      5  0.804315  absolutely\n",
       "418       2  0.999981         add\n",
       "152       1  0.432215        also\n",
       "...     ...       ...         ...\n",
       "696       5  0.230160        work\n",
       "122       1  0.999334       would\n",
       "122       3  0.000655       would\n",
       "243       3  0.899365       years\n",
       "243       4  0.100606       years\n",
       "\n",
       "[310 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 2, 5, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "print(data_words_bigrams[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "data_words_trigrams = make_trigrams(data_words)\n",
    "print(data_words_trigrams[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The first 10 words in our dictionary are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "vocabulary = Dictionary(data_words_bigrams)\n",
    "\n",
    "vocabulary_keys = list(vocabulary.token2id)[0:10]\n",
    "\n",
    "for key in vocabulary_keys:\n",
    "    print(f\"ID: {vocabulary.token2id[key]}, Token: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for a machine learning model to work with text input, the document must first be vectorized. So the input has to be converted into  numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical approach in expressing text as a set of features is getting the token frequency. Each entry to the dataframe is a document while each column corresponds to every unique token in the entire corpora. The row will identify how many times a word appears in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [vocabulary.doc2bow(doc) for doc in data_words_bigrams]\n",
    "\n",
    "for idx, freq in bow[0]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(bow)\n",
    "\n",
    "for idx, weight in tfidf[bow[0]]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Weight: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcomings of count-based feature engineering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional count-based feature engineering models are bag of words and  TF-IDF. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence and context around nearby words in each text document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The need for word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of raw text data, especially count based models like Bag of Words, we are dealing with individual words which may have their own identifiers and do not capture the semantic relationship amongst words. This leads to huge sparse word vectors for textual data and thus if we do not have enough data, we may end up getting poor models or even overfitting the data due to the curse of dimensionality.\n",
    "\n",
    "Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process it learns distributed representations giving us dense word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity. Essentially these are unsupervised models which can take in massive textual corpora, create a vocabulary of possible words and generate dense word embeddings for each word in the vector space representing that vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# Set values for various parameters\n",
    "feature_size = 100    # Word vector dimensionality  \n",
    "window_context = 30          # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(data_words, size=feature_size, \n",
    "                          window=window_context, min_count=min_word_count,\n",
    "                          sample=sample, iter=50,seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view similar words based on gensim's model\n",
    "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
    "                  for search_term in ['best', 'like', 'use', 'product', 'horrible', 'taste','price','dog']}\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better appreciate the concept of word embeddings, I took eight common words in our corpora and derive their five most related words using our word_vec model. The similarity comes from how often these tokens appear in the same window of words as their similar_words counterpart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data.  t-SNE gives a feel or intuition of how the data is arranged in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
    "wvs = w2v_model.wv[words]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(wvs)\n",
    "labels = words\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(T[:, 0], T[:, 1])#, c='orange', edgecolors='r')\n",
    "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label, xy=(x+3, y+3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Word2Vec characterizes words into quantified tokens, we can consequently add or subtract word vectors together. To add is to combine the meaning of the components and to subtract is to take out the context of one token from another. The following are examples of this vector algebra and their similarity scores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dog + Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"dog\", \"food\"], \\\n",
    "                      negative=[], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coffee + tea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"coffee\",'tea'], \\\n",
    "                      negative=[], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow,\n",
    "                                           id2word=vocabulary,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(0, 5):\n",
    "    print(f\"\\nTopic {topic+1}:\")\n",
    "    for token, frequency in lda_model.show_topic(topic, topn=10):\n",
    "        print(f\" {token}, {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means the top 10 keywords that contribute to this topic 3 are: ‘taste’, ‘like’, ‘good’.. and so on and the weight of ‘taste’ on topic 3 is 0.05.\n",
    "The weights reflect how important a keyword is to that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_idm = pyLDAvis.gensim.prepare(lda_model, bow, vocabulary)\n",
    "\n",
    "pyLDAvis.display(lda_idm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "The right-hand side bars represent the words,form the selected topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
